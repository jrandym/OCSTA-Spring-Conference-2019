{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to DS - Spring Conference.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrandym/OCSTA-Spring-Conference-2019/blob/master/Intro_to_DS_Spring_Conference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BJokzbmKRAoJ"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data Science - OCSTA Spring Conference\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yLa9zX6_fSLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Instructor:**  \n",
        "Sylvana Yelda  \n",
        "*Sr. Data Scientist, Kollective*  \n",
        "syelda@gmail.com"
      ]
    },
    {
      "metadata": {
        "id": "tYWmOq5rjPlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: Please copy this notebook so that you can edit your own version. Go to File --> Save a copy in Drive. Then you may close the original version, and rename your copy to whatever you would like."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xFverULPRAoL"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "B9PK4yNnRAoN"
      },
      "cell_type": "markdown",
      "source": [
        "Data science combines statistics, data analysis and visualization, and machine learning with a goal of analyzing and extracting insight from large amounts of data. \n",
        "\n",
        "In this notebook, you will get a brief introduction to a typical data science workflow using the programming language, Python. Don't worry if you aren't familiar with Python, or even programming in general. The goal of this hour-long course is to give you a basic understanding of the steps a data scientist might take when trying to answer a specific question with data. During the 3-day course, we will go into more details of each of these steps, including a tutorial on Python.\n",
        "\n",
        "<b>Python</b> is an open-source general-purpose programming language used in a wide variety of applications. Python was created by Guido Van Rossum in the early 1990s and is now used extensively at places like NASA, Google, Amazon, Netflix, YouTube, and Apple. \n",
        "\n",
        "<b>Google Colaboratory</b> is a research tool for data science and machine learning education and collaboration. It is based on the open-source <b>Jupyter Notebooks</b>, interactive documents that include text and code. The documents are ordinary files with a suffix .ipynb. They can be uploaded, downloaded, and shared like any other digital document. The notebooks are composed of individual cells containing either text, programming code, or the output of a calculation. The code cells can contain code written in many different programming languages such as Python, R, and Javascript."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DhfboKE-RAoN"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook Basics"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LOfN3V5PRAoP"
      },
      "cell_type": "markdown",
      "source": [
        "* Each cell in this notebook can either contain text or code. This cell contains text called 'Markdown'. \n",
        "\n",
        "* You can \"run\" a cell (whether it has text or code) in a few different ways:\n",
        "- with the cell selected, press either:\n",
        "    - Shift-Enter\n",
        "    - Alt-Enter (this will run the cell then insert a new cell below)\n",
        "    \n",
        "* Code cells can also be run by clicking the 'Run Cell' icon on the very left of the cell.\n",
        "   \n",
        "* New cells can be inserted above or below existing cells. This can be done from the Insert menu or by pointing your mouse to just below or above the middle of an existing cell.\n",
        "\n",
        "* Don't forget to save your work from time to time!"
      ]
    },
    {
      "metadata": {
        "id": "vhMFSz46FeoL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What we'll do today"
      ]
    },
    {
      "metadata": {
        "id": "ICYbU7aoF3g_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The steps taken in any data science project typically are not linear, and you may go back and forth between some steps. Below are many of the most common tasks, some of which we will cover today:\n",
        "\n",
        "\n",
        "*   define question\n",
        "*   gather and prepare data\n",
        "*   exploratory analysis\n",
        "*   machine learning\n",
        "*   evaluate results\n",
        "*   present findings"
      ]
    },
    {
      "metadata": {
        "id": "pJwLSq-HA_IV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Problem"
      ]
    },
    {
      "metadata": {
        "id": "Cg12u1CCBVwB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Very broadly, machine learning can be split into regression problems and classification problems. *Today, we will build a machine learning model that classifies iris flowers based on a few characteristics.* We will use the well-known \"iris dataset\", which contains 150 observations of iris flowers. \n",
        "\n",
        "Three types of iris flowers are found in the dataset: \n",
        "* iris setosa\n",
        "* iris  virginica\n",
        "* iris versicolor\n",
        "\n",
        "Each observation contains the following measurements in centimeters (cm):\n",
        "* sepal length\n",
        "* sepal width\n",
        "* petal length\n",
        "* petal width\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "h9UeloR8DijT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Data"
      ]
    },
    {
      "metadata": {
        "id": "URXLXE44DVuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCxsftFos8GC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's read in the data. The location of the file is below. Create a variable called 'url' with the link stored as a string. Then read it using pandas' read_csv() method.\n",
        "\n",
        "https://raw.githubusercontent.com/syelda/ocsta-intro-to-ds/master/iris_data_intro_to_ds.csv\n",
        "\n",
        "Call the data 'data'. "
      ]
    },
    {
      "metadata": {
        "id": "t6lVPzgj2lOO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2PVZatAxRiy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we first load up a dataset, we want to start exploring the data by printing out the data and various metrics, as well as visualizing the data. Data is almost always messy, and we need to understand it and identify any potential problems before beginning any analysis. Some of the things we might look for are any missing data, weird data points, and outliers, just to name a few.\n",
        "\n",
        "Let's start by examining the top 5 rows of the dataset using the head() method."
      ]
    },
    {
      "metadata": {
        "id": "eDNEGQiOt5gR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HNk5ONt-1t_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A quick sidenote about dataframes"
      ]
    },
    {
      "metadata": {
        "id": "RPFmU3vO1yEE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataframes are a common tool used in data science. A dataframe is like a spreadsheet. It is simply a table filled with data, where each row represents a single record (or observation), and each column represents a field. Dataframes make it very easy to work with data at all stages of data science.\n",
        "\n",
        "Subsets of a dataframe or individual columns can be extracted using square brackets:  \n",
        "* df[['column1', 'column2']]  # notice the double brackets -- this returns a dataframe\n",
        "* df['column1']  # single brackets returns a series (or column)\n",
        "\n",
        "Methods (i.e., functions) can be applied to dataframes using the 'dot' notation:  \n",
        "* df.head()\n",
        "* df.describe()\n",
        "\n",
        "Finally, dataframes can be filtered by using conditions in the square brackets:  \n",
        "* df.loc[df['column1'] > 5]\n",
        "* df.loc[(df['column1'] > 5) & (df['column2'] == 'blue')]\n",
        "* df.loc[df['column1'] > 5, 'column2']\n",
        "\n",
        "---\n",
        "\n",
        "Ok, back to the analysis..."
      ]
    },
    {
      "metadata": {
        "id": "y2p9A7cLnVZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is important to know if we have any missing data, as this can affect our models later on. We can use the isnull() and sum() methods together on the dataset to know how many data points are missing."
      ]
    },
    {
      "metadata": {
        "id": "jvxdNV3aD4NA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CjjAOH1ZnPE-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at the rows that have missing values."
      ]
    },
    {
      "metadata": {
        "id": "td3FKiu3iaJ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frOKAMYfn2AI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The describe() method is a quick way to get summary statistics on our dataframe."
      ]
    },
    {
      "metadata": {
        "id": "6WOETgH4vHQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dtig_nNdn8Zm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualizations can be very insightful at this stage of our analysis. Let's plot every variable against every other variable. This can be done easily using seaborn's pairplot."
      ]
    },
    {
      "metadata": {
        "id": "dnGRpHHPvIJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivf_P4GZvUBI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note that you have to drop NAs for this to work\n",
        "sns.pairplot(data.dropna(), hue='class');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--qAjW2LwBXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What are some observations you can make from the pairplot above? Any issues you can identify?"
      ]
    },
    {
      "metadata": {
        "id": "6dPKgXjWsz2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "lD6QV-0Ay-e0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Wrangling"
      ]
    },
    {
      "metadata": {
        "id": "C2S5HYi062S4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First let's address the outliers. We saw that there were several sepal length data points for the iris versicolor flower that have near zero values. Let's examine these further and decide what to do with them."
      ]
    },
    {
      "metadata": {
        "id": "6xSOOwOD7W9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[data['class'] == 'Iris-versicolor', 'sepal_length_cm'].hist();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HkrLBYrH1wN9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[(data['class'] == 'Iris-versicolor') &\n",
        "         (data['sepal_length_cm'] < 1.0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPqnrCy-7b6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Any ideas what may be going on here? Do you see anything in common amongst these 5 data points?"
      ]
    },
    {
      "metadata": {
        "id": "TM_8CkNA7qZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Could these observations have been recorded in units of meters instead of centimeters? It looks like that may be the case. \n",
        "\n",
        "Let's say we checked our hunch with the field researchers and we found that this is indeed what happened. Now we can fix this error in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "h8abVMCc7NZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[((data['class'] == 'Iris-versicolor') &\n",
        "         (data['sepal_length_cm'] < 1.0)),\n",
        "         'sepal_length_cm'] *= 100.0\n",
        "\n",
        "data.loc[data['class'] == 'Iris-versicolor', 'sepal_length_cm'].hist();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I_LiVFNj8lh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's handle the observations with missing data. Recall that we were missing 5 data points for petal_width_cm."
      ]
    },
    {
      "metadata": {
        "id": "tiBNTg9M8wr9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[data['petal_width_cm'].isnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vRnwLrW9E3t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All of the missing points are for iris setosa and for the same type of measurement (petal width). It would not be ideal to just remove these, as this could potentially bias our analysis. Instead, we can fill in the missing data (a process known as 'data imputation'). There are many clever ways of doing this. For this exercise, we will use mean imputation. If we know that the values for a measurement fall in a certain range, we can fill in empty values with the average of that measurement."
      ]
    },
    {
      "metadata": {
        "id": "lQNsdvTR-iIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "average_petal_width = data.loc[data['class'] == 'Iris-setosa', \n",
        "                               'petal_width_cm'].mean()\n",
        "\n",
        "data.loc[((data['class'] == 'Iris-setosa') &\n",
        "         (data['petal_width_cm'].isnull())),\n",
        "         'petal_width_cm'] = average_petal_width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GPZEBqevO7lA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Print out the rows for which we just filled in data."
      ]
    },
    {
      "metadata": {
        "id": "ESJWFxphO7Ks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[(data['class'] == 'Iris-setosa') &\n",
        "         (data['petal_width_cm'] == average_petal_width)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0zE1QZS-Dfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check for null values one more time."
      ]
    },
    {
      "metadata": {
        "id": "CqiA8tge-uYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggyYqVOFEKjb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at our data one more time now that it's all cleaned up!"
      ]
    },
    {
      "metadata": {
        "id": "8fLce_YGAN4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.pairplot(data, hue='class');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKnWicVQAdFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's review the general takeaways so far.\n",
        "\n",
        "* Examine the observed ranges and compare with the expected ranges (if possible); it is OK to use domain knowledge whenever possible to define that expected range\n",
        "\n",
        "* Address missing data in one way or another; data can be replaced or dropped, but justification is needed\n",
        "\n",
        "* Never clean/transform your data manually as that is not easily reproducible; always use code as a record of how you cleaned your data\n",
        "\n",
        "* Visualize as much as you can about the data at this stage of the analysis so you can confirm everything looks correct"
      ]
    },
    {
      "metadata": {
        "id": "y39ccBgmC6iw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exploratory Analysis"
      ]
    },
    {
      "metadata": {
        "id": "Z3HcqPRODLq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now to the fun part! We can start exploring the data at a deeper level using visualizations and statistics, as needed. This will give us the insight we may need while building and interpreting our machine learning model. Some of the questions we want to answer here include:\n",
        "* how are my data distributed?\n",
        "* are there any correlations in the data?\n",
        "* are there any confounding factors that may explain the apparent correlations?"
      ]
    },
    {
      "metadata": {
        "id": "c2TtkLEkEhj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start by plotting the pairplot again, this time without any color-coding."
      ]
    },
    {
      "metadata": {
        "id": "UF6pR5nIEhIm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4HdXGmsfHruZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Much of the data appear to be normally distributed, which may be important if any models we use assume a normal distribution. However, there are some interesting features in the petal distributions. Could this be because of the different species? Let's look at this again but color-code by species."
      ]
    },
    {
      "metadata": {
        "id": "TwSPrIkWIF-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ufy1FtLOEqZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The strange distributions in the petal measurements appear to be related to the fact that we've plotted data for different distributions. This is important for our classification model! We see just with this plot that Iris-setosa has a completely different distribution in petal width and length as compared to the other 2 species. This will really help our model distinguish between the species. However, our model may have a slightly harder time distinguishing between Iris-versicolor and Iris-virginica, whose distributions have more of an overlap."
      ]
    },
    {
      "metadata": {
        "id": "9Lc39XLJHNvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the plot above, can you identify any possible correlations? It looks like there are correlations between petal length and petal width, as well as sepal length and sepal width. "
      ]
    },
    {
      "metadata": {
        "id": "GbYYLgC4D5Sw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a Model with ML"
      ]
    },
    {
      "metadata": {
        "id": "m-5QoQJLHrp1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are finally at the stage where we can model the data! The large majority of a data scientist's time is spent gathering, cleaning, and understanding the data. Only after those critical steps are completed do we actually start any work on modeling. Bad data will only lead to bad models, in other words, **garbage in = garbage out**."
      ]
    },
    {
      "metadata": {
        "id": "aNTQmBzkIzo8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When building a machine learning model, we must split our dataset into a training set and a test set. \n",
        "\n",
        "* A training set is a subset of the data, selected randomly, that is used to train our models.\n",
        "\n",
        "* A testing set is the remaining subset of the data (mutually exclusive from the training set) that is used to validate (or 'score') our models. It is critical that you test your model with new data (i.e., data that was *not* used to build the model)."
      ]
    },
    {
      "metadata": {
        "id": "cpPtwe0LEA8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We can extract the data in this format from pandas like this:\n",
        "all_inputs = data[['sepal_length_cm', 'sepal_width_cm',\n",
        "                   'petal_length_cm', 'petal_width_cm']]\n",
        "\n",
        "# Similarly, we can extract the class labels\n",
        "all_labels = data['class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xO0gwSLdOEPy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at a subset of the inputs. Print out the first 5 entries of all_inputs."
      ]
    },
    {
      "metadata": {
        "id": "m43r8zGdHkKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TWZ0vGZaKD7x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's split the data now."
      ]
    },
    {
      "metadata": {
        "id": "v-A3acl6J8C9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(training_inputs,\n",
        " testing_inputs,\n",
        " training_classes,\n",
        " testing_classes) = train_test_split(all_inputs, all_labels, \n",
        "                                     test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWQYaftJK2Un",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are many different types of models that we can use for this problem. We will use decision tree classifiers today, as they are simple to understand and yet can be quite powerful. \n",
        "\n",
        "Decision trees can be thought of as a series of yes/no questions about the data, each time getting closer to finding out the class of each entry — until they either classify the data set perfectly or simply can't differentiate a set of entries. \n",
        "\n",
        "Decision tree classifiers can take many different parameters. But for the purposes of this workshop, we will just use a basic decision tree.\n",
        "\n",
        "More info can be found here:  \n",
        "https://scikit-learn.org/stable/modules/tree.html"
      ]
    },
    {
      "metadata": {
        "id": "8_CHSfa_Km3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create ('instantiate') the classifier\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "decision_tree_classifier.fit(training_inputs, training_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ml0VDXVIPrKH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to validate the classifier on the testing set using classification accuracy."
      ]
    },
    {
      "metadata": {
        "id": "rxTcztUFM-aA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decision_tree_classifier.score(testing_inputs, testing_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OO7NTyzcMrf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wow! Our simple model scored very high! Let's think about this for a minute. We used a random subset of 80% of our data to train our model. What if we used a different subset? What if one subset of our data has mostly data for two species of flower and not the third? This could result in overfitting.\n",
        "\n",
        "This is where cross validation comes in."
      ]
    },
    {
      "metadata": {
        "id": "_8N8uo_EfKjp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**k-fold cross validation** involves partitioning the original dataset into k parts of equal or near-equal size, each called a fold. A series of k models is trained, one per fold. e.g., in a 10-fold, model 1 is trained using folds 2-10 as the training set, and evaluated using fold 1 as the test set, and so on. So every data point is used to test the model. This will result in 10 accuracy values, 1 per fold. In other words, cross validation measures the effectiveness of your model.\n",
        "\n",
        "https://scikit-learn.org/stable/_images/grid_search_cross_validation.png"
      ]
    },
    {
      "metadata": {
        "id": "ls9uZhgZihB8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# cross_val_score returns a list of the scores, which we can visualize\n",
        "# to get a reasonable estimate of our classifier's performance\n",
        "cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, \n",
        "                            cv=10)\n",
        "plt.hist(cv_scores)\n",
        "plt.title('Average score: {:.2f} +/- {:.2f}'.format(np.mean(cv_scores), np.std(cv_scores)));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OcBp3KjLjJrM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With k-fold cross validation, we assess that our classifier has a mean classification accuracy of 0.96.\n",
        "\n",
        "Next steps that could be taken (if we only had the time!):\n",
        "\n",
        "* parameter tuning using **grid search** \n",
        "* train other types of models and compare to our decision tree classifier"
      ]
    },
    {
      "metadata": {
        "id": "1SZg2GV4jXUO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusions"
      ]
    },
    {
      "metadata": {
        "id": "wMIlcptNjcXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We built a simple, but quite accurate, machine learning model that can predict a species of Iris flower based on a set of measurements. We learned about the general process a data scientist goes through and many of the tools used in data projects. I hope you found it fun and interesting!"
      ]
    },
    {
      "metadata": {
        "id": "saO6sqG85NO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Acknowledgements"
      ]
    },
    {
      "metadata": {
        "id": "pGreYHIH5Rsc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some of the analysis in this notebook was adapted from educational material created by Randal S. Olson under the [Creative Commons Attribution License](https://creativecommons.org/licenses/by/4.0/). "
      ]
    }
  ]
}